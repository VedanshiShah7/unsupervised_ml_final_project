{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 20.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.37MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.98MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch 1/10, Loss: 1.0305\n",
      "Epoch 2/10, Loss: 0.9254\n",
      "Epoch 3/10, Loss: 0.9254\n",
      "Epoch 4/10, Loss: 0.9254\n",
      "Epoch 5/10, Loss: 0.9254\n",
      "Epoch 6/10, Loss: 0.9254\n",
      "Epoch 7/10, Loss: 0.9254\n",
      "Epoch 8/10, Loss: 0.9254\n",
      "Epoch 9/10, Loss: 0.9254\n",
      "Epoch 10/10, Loss: 0.9254\n",
      "K-Means++ Accuracy: 0.0980, AUC: 0.5000\n",
      "DBSCAN Accuracy: 0.1103, AUC: 0.5018\n",
      "One-Class SVM Accuracy: 0.0980, AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 128\n",
    "NORMAL_CLASS = 0\n",
    "EPOCHS = 10\n",
    "LATENT_DIM = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "def filter_normal(dataset, normal_class):\n",
    "    imgs, labels = [], []\n",
    "    for img, label in dataset:\n",
    "        imgs.append(img)\n",
    "        labels.append(int(label == normal_class))  # 1 for normal, 0 for anomaly\n",
    "    return torch.stack(imgs), torch.tensor(labels)\n",
    "\n",
    "train_imgs, _ = filter_normal(train_data, NORMAL_CLASS)\n",
    "test_imgs, test_labels = filter_normal(test_data, NORMAL_CLASS)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Convolutional Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # [16, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, 3, stride=2, padding=1),  # [4, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 7 * 7, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4 * 7 * 7),\n",
    "            nn.Unflatten(1, (4, 7, 7)),\n",
    "            nn.ConvTranspose2d(4, 16, 3, stride=2, padding=1, output_padding=1),  # [16, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  # [1, 28, 28]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "model = Autoencoder(latent_dim=LATENT_DIM).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch)\n",
    "        loss = criterion(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Feature Extraction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_features = model.encode(train_imgs.to(DEVICE)).cpu().numpy()\n",
    "    test_features = model.encode(test_imgs.to(DEVICE)).cpu().numpy()\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "# Clustering in Latent Space\n",
    "# K-Means\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
    "kmeans.fit(train_features)\n",
    "kmeans_preds = kmeans.predict(test_features)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_preds = dbscan.fit_predict(test_features)\n",
    "\n",
    "# One-Class SVM\n",
    "oc_svm = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\n",
    "oc_svm.fit(train_features)\n",
    "svm_preds = oc_svm.decision_function(test_features)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(preds, true_labels):\n",
    "    preds = np.where(preds == -1, 0, 1)  # Map -1 to 0 for anomalies\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    auc = roc_auc_score(true_labels, preds)\n",
    "    return acc, auc\n",
    "\n",
    "kmeans_acc, kmeans_auc = evaluate(kmeans_preds, test_labels.numpy())\n",
    "dbscan_acc, dbscan_auc = evaluate(dbscan_preds, test_labels.numpy())\n",
    "svm_acc, svm_auc = evaluate(svm_preds, test_labels.numpy())\n",
    "\n",
    "print(f\"K-Means++ Accuracy: {kmeans_acc:.4f}, AUC: {kmeans_auc:.4f}\")\n",
    "print(f\"DBSCAN Accuracy: {dbscan_acc:.4f}, AUC: {dbscan_auc:.4f}\")\n",
    "print(f\"One-Class SVM Accuracy: {svm_acc:.4f}, AUC: {svm_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
